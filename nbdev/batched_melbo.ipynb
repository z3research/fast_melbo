{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a815b51c",
   "metadata": {},
   "source": [
    "# BatchedMELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d75624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp batched_melbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2c01f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export \n",
    "#| eval:false\n",
    "#| hide\n",
    "from __future__ import annotations\n",
    "import torch.nn.functional as F\n",
    "import torch as t\n",
    "import torch\n",
    "from jaxtyping import *\n",
    "from tqdm import tqdm\n",
    "import functools, tqdm\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfc512",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class hooks():\n",
    "    def __init__(self, model, hooks: list[tuple[torch.nn.Module, callable]]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: The model to hook\n",
    "            hooks: A list of tuples of the form (module, hook_fn)\n",
    "                module: The module to hook\n",
    "                hook_fn: The function to call when the hook is triggered. Should take the input and return the modified input.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.handles = []\n",
    "        self.hooks = hooks\n",
    "\n",
    "    def __enter__(self):\n",
    "        for module, hook_fn in self.hooks:\n",
    "            def post_hook(m, input, output):\n",
    "                if isinstance(output, tuple):\n",
    "                    modified_output = hook_fn(output[0])\n",
    "                    return (modified_output,) + output[1:]\n",
    "\n",
    "                return hook_fn(output)\n",
    "\n",
    "            self.handles.append(module.register_forward_hook(post_hook))\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c78c8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def easy_generate(model, tokenizer, prompts: list[str], **kwargs):\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding=True).to(model.device)\n",
    "    generations = model.generate(**inputs, **kwargs)\n",
    "    return tokenizer.batch_decode(generations, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cebd4d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def easy_forward(model, tokenizer, prompts: list[str], **kwargs):\n",
    "    tokens = tokenizer(prompts, return_tensors='pt', padding=True).to(model.device)\n",
    "    return model(**tokens, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73282f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rgetattr(obj, path):\n",
    "    return functools.reduce(getattr, path.split(\".\"), obj)\n",
    "\n",
    "def project_orthogonal_subspace(vec, learned_vectors, normalization):\n",
    "    U = learned_vectors.t() / normalization\n",
    "    result = vec - U @ U.t() @ vec\n",
    "    return result\n",
    "\n",
    "class BatchedMELBO():\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        tokenizer, \n",
    "        source_layer_idx=None, \n",
    "        target_layer_idx=None, \n",
    "        target_token_idxs=slice(None), \n",
    "        layers_name=None, \n",
    "        normalization=1.0, \n",
    "        num_steps=300, \n",
    "        power=2, \n",
    "        q=None\n",
    "        ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # determine layers object\n",
    "        if layers_name is None:\n",
    "            if hasattr(self.model, \"transformer\"):  # gpt-2-like\n",
    "                self.layers_name = \"transformer.h\"\n",
    "            elif hasattr(self.model, \"gpt_neox\"): # pythia-like\n",
    "                self.layers_name = \"gpt_neox.layers\"\n",
    "            elif hasattr(self.model, \"model\"):  # mistral-like\n",
    "                self.layers_name =  \"model.model.layers\"\n",
    "            elif hasattr(self.model, \"layers\"):  # qwen2-like\n",
    "                self.layers_name =  \"model.layers\"\n",
    "            else:\n",
    "                raise ValueError(f\"don't know how to get layer list for {type(model)}\")\n",
    "        else:\n",
    "            self.layers_name = layers_name\n",
    "        self.layers = rgetattr(self.model, self.layers_name)\n",
    "        \n",
    "        # determine source layer\n",
    "        if source_layer_idx is None:\n",
    "            self.source_layer_idx = 7\n",
    "        else:\n",
    "            self.source_layer_idx = source_layer_idx\n",
    "        \n",
    "        # determine target layer\n",
    "        if target_layer_idx is None:\n",
    "            self.target_layer_idx = len(self.layers) - 8\n",
    "        else:\n",
    "            self.target_layer_idx = target_layer_idx\n",
    "        \n",
    "        # get width\n",
    "        self.width = model.config.hidden_size\n",
    "        \n",
    "        # set other hyper-parameters\n",
    "        self.normalization = normalization\n",
    "        self.target_token_idxs = target_token_idxs\n",
    "        self.num_steps = num_steps\n",
    "        self.power = power\n",
    "        if q is None:\n",
    "            self.q = self.power\n",
    "        else:\n",
    "            self.q = q\n",
    "\n",
    "        # don't need to store grads for parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def train(self, examples, num_vectors, vector_batch_size=128):\n",
    "        if isinstance(examples, str):\n",
    "            examples = [examples]\n",
    "        \n",
    "        enable_autocast = self.model.dtype in (torch.bfloat16, torch.float16)\n",
    "        self.num_vectors = num_vectors\n",
    "        # initialize with random vectors\n",
    "        self.learned_vectors = torch.randn(self.num_vectors, self.width, device=self.model.device)\n",
    "        self.learned_vectors = nn.functional.normalize(self.learned_vectors, dim=-1) * self.normalization\n",
    "\n",
    "        num_steps = self.num_steps\n",
    "        normalization = self.normalization\n",
    "        power = self.power\n",
    "        \n",
    "        # compute unsteered targets\n",
    "        model_inputs = self.tokenizer(examples, return_tensors=\"pt\", padding=True).to(self.model.device)\n",
    "        with torch.no_grad(), torch.autocast(device_type=self.model.device.type, dtype=self.model.dtype, enabled=enable_autocast):\n",
    "            hidden_states = self.model(**model_inputs, output_hidden_states=True).hidden_states\n",
    "        unsteered_targets = hidden_states[self.target_layer_idx+1][:, self.target_token_idxs, :]\n",
    "        \n",
    "        # loop over vectors\n",
    "        losses_all = torch.zeros(num_vectors, num_steps)\n",
    "\n",
    "        for batch_start in range(0, num_vectors, vector_batch_size):\n",
    "            batch_end = min(batch_start + vector_batch_size, num_vectors)\n",
    "            batch_size = batch_end - batch_start\n",
    "\n",
    "            repeated_unsteered_targets = unsteered_targets.repeat(batch_size, 1, 1)\n",
    "            \n",
    "            biases = torch.zeros(batch_size, 1, self.width, device=self.model.device, requires_grad=True)\n",
    "\n",
    "             # initialize\n",
    "            # batch_losses = []\n",
    "            with torch.no_grad():\n",
    "                biases.data = normalization*nn.functional.normalize(torch.randn(batch_size, 1, self.width, device=self.model.device), dim=-1)\n",
    "       \n",
    "            optimizer = optim.AdamW([biases], lr=.001, betas=(.9,.98), weight_decay=0.0, amsgrad=True)\n",
    "\n",
    "            model_inputs = self.tokenizer(examples*batch_size, return_tensors=\"pt\", padding=True).to(self.model.device)\n",
    "\n",
    "            # training loop\n",
    "            for t in tqdm.tqdm(range(num_steps), desc=f\"Training batch {((batch_start+1)//vector_batch_size)+1} of {(num_vectors//vector_batch_size)+1}\"):\n",
    "                # compute gradient\n",
    "                optimizer.zero_grad()    \n",
    "\n",
    "                # compute steered target\n",
    "                with torch.autocast(device_type=self.model.device.type, dtype=self.model.dtype, enabled=enable_autocast), self.steer(biases.repeat_interleave(len(examples), dim=0)):\n",
    "                    hidden_states = self.model(**model_inputs, output_hidden_states=True).hidden_states\n",
    "                    target = hidden_states[self.target_layer_idx+1][:, self.target_token_idxs, :] # batch, pos, width\n",
    "                    \n",
    "                    loss = -(target - repeated_unsteered_targets).norm(dim=-1).pow(power).sum(dim=-1).pow(1/self.q) # batch\n",
    "                    with torch.no_grad():\n",
    "                        losses_all[batch_start:batch_end, t] = loss.data.detach().clone().view(len(examples), -1).mean(dim=0)\n",
    "                loss.sum().backward()\n",
    "\n",
    "                # project gradient to tangent space of sphere\n",
    "                with torch.no_grad():\n",
    "                    for bias_idx in range(batch_size):\n",
    "                        biases.grad[bias_idx] -= torch.dot(\n",
    "                            input=biases.grad[bias_idx, 0], \n",
    "                            tensor=biases[bias_idx, 0]\n",
    "                        ) * biases[bias_idx] / (normalization**2)\n",
    "                \n",
    "                # step\n",
    "                optimizer.step()\n",
    "\n",
    "                # normalize\n",
    "                with torch.no_grad():\n",
    "                    biases.data = nn.functional.normalize(biases.data, dim=-1) * normalization\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                self.learned_vectors[batch_start:batch_end] = biases.data.detach()[:, 0]\n",
    "\n",
    "        self.losses_all = losses_all.tolist()\n",
    "\n",
    "    def steer(self, vector: int | Float[t.Tensor, \"batch 1 width\"]):\n",
    "        vector = self.learned_vectors[vector] if isinstance(vector, int) else vector\n",
    "        return hooks(self.model, [\n",
    "            (self.layers[self.source_layer_idx], lambda z: z+vector.to(z.dtype))\n",
    "        ])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py,../nbdev//ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
