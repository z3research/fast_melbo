{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15cbc07b",
   "metadata": {},
   "source": [
    "# OriginalMELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp original_melbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea4cf3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "import functools, tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a674e808",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rgetattr(obj, path):\n",
    "    return functools.reduce(getattr, path.split(\".\"), obj)\n",
    "\n",
    "def project_orthogonal_subspace(vec, learned_vectors, normalization):\n",
    "    U = learned_vectors.t() / normalization\n",
    "    result = vec - U @ U.t() @ vec\n",
    "    return result\n",
    "\n",
    "class SteeredModel():\n",
    "    def __init__(self, model, tokenizer, source_layer_idx=None, target_layer_idx=None, target_token_idxs=slice(None), layers_name=None, source_module_name=None, normalization=1.0, num_steps=300, power=2, q=None, orthogonal_vectors=False, target_module=\"residual\"):\n",
    "        '''\n",
    "        Note: this will mutate `model`\n",
    "        '''\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "            \n",
    "        # determine layers object\n",
    "        if layers_name is None:\n",
    "            if hasattr(self.model, \"transformer\"):  # gpt-2-like\n",
    "                self.layers_name = \"transformer.h\"\n",
    "            elif hasattr(self.model, \"gpt_neox\"): # pythia-like\n",
    "                self.layers_name = \"gpt_neox.layers\"\n",
    "            elif hasattr(self.model, \"model\"):  # mistral-like\n",
    "                self.layers_name =  \"model.model.layers\"\n",
    "            else:\n",
    "                raise ValueError(f\"don't know how to get layer list for {type(model)}\")\n",
    "        else:\n",
    "            self.layers_name = layers_name\n",
    "        self.layers = rgetattr(self.model, self.layers_name)\n",
    "        \n",
    "        # determine source layer\n",
    "        if source_layer_idx is None:\n",
    "            self.source_layer_idx = 7\n",
    "        else:\n",
    "            self.source_layer_idx = source_layer_idx\n",
    "        \n",
    "        # determine target layer\n",
    "        if target_layer_idx is None:\n",
    "            self.target_layer_idx = len(self.layers) - 8\n",
    "        else:\n",
    "            self.target_layer_idx = target_layer_idx\n",
    "        \n",
    "        # determine source_module_name\n",
    "        if source_module_name is None:\n",
    "            if \"QWen\" in type(self.model).__name__:\n",
    "                self.source_module_name = \"mlp.c_proj\"\n",
    "            elif hasattr(self.model, \"gpt_neox\"):\n",
    "                self.source_module_name = \"mlp.dense_4h_to_h\"\n",
    "            else:\n",
    "                self.source_module_name = \"mlp.down_proj\" # otherwise guess \"down_proj\"\n",
    "        else:\n",
    "            self.source_module_name = source_module_name\n",
    "            \n",
    "        # get width\n",
    "        self.width = rgetattr(self.layers[0], self.source_module_name).out_features\n",
    "        \n",
    "        # set other hyper-parameters\n",
    "        self.normalization = normalization\n",
    "        self.target_token_idxs = target_token_idxs\n",
    "        self.num_steps = num_steps\n",
    "        self.power = power\n",
    "        if q is None:\n",
    "            self.q = self.power\n",
    "        else:\n",
    "            self.q = q\n",
    "        self.orthogonal_vectors = orthogonal_vectors\n",
    "        self.target_module = target_module\n",
    "\n",
    "        # don't need to store grads for most parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # set bias\n",
    "        rgetattr(rgetattr(self.model, f\"{self.layers_name}\")[self.source_layer_idx], self.source_module_name).bias = nn.Parameter(\n",
    "            torch.zeros(self.width, device=self.model.device)\n",
    "        )\n",
    "        self.bias = rgetattr(rgetattr(self.model, f\"{self.layers_name}\")[self.source_layer_idx], self.source_module_name).bias\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def train(self, examples, num_vectors):\n",
    "        self.num_vectors = num_vectors\n",
    "        self.learned_vectors = torch.zeros(self.num_vectors, self.width, device=self.model.device)\n",
    "\n",
    "        num_steps = self.num_steps\n",
    "        orthogonal_vectors = self.orthogonal_vectors\n",
    "        normalization = self.normalization\n",
    "        power = self.power\n",
    "        \n",
    "        # compute unsteered targets\n",
    "        self.zero_steering_vector()\n",
    "        self.unsteered_targets = []\n",
    "        for i in range(len(examples)):\n",
    "            model_inputs = self.tokenizer([examples[i]], return_tensors=\"pt\", padding=False).to(self.model.device)\n",
    "            with torch.no_grad():\n",
    "                if self.target_module == \"residual\":\n",
    "                    hidden_states = self.model(model_inputs[\"input_ids\"], output_hidden_states=True).hidden_states\n",
    "                elif self.target_module == \"attn\":\n",
    "                    hidden_states = self.model(model_inputs[\"input_ids\"], output_attentions=True).attentions\n",
    "                else:\n",
    "                    raise ValueError(\"target_module must be 'residual' or 'attn'\")\n",
    "                self.unsteered_targets.append(hidden_states[self.target_layer_idx][:, self.target_token_idxs, :])\n",
    "\n",
    "        \n",
    "        # loop over vectors\n",
    "        losses_all = []\n",
    "        bias = self.bias\n",
    "        for i in (pbar := tqdm.tqdm(range(num_vectors))):\n",
    "            \n",
    "            # initialize\n",
    "            losses = []\n",
    "            with torch.no_grad():\n",
    "                if self.orthogonal_vectors:\n",
    "                    bias.data = normalization*nn.functional.normalize(\n",
    "                        project_orthogonal_subspace(\n",
    "                            torch.randn(self.width, device=\"cuda\"), self.learned_vectors, self.normalization\n",
    "                        ), \n",
    "                        dim=0\n",
    "                    )\n",
    "                else:\n",
    "                    bias.data = normalization*nn.functional.normalize(torch.randn(self.width, device=\"cuda\"), dim=0)\n",
    "                        \n",
    "            # optimizer\n",
    "            optimizer = optim.AdamW([bias],\n",
    "                                    lr=.001, betas=(.9,.98), weight_decay=0.0, amsgrad=True)\n",
    "            \n",
    "            # training loop\n",
    "            for t in range(num_steps):\n",
    "                \n",
    "                # compute gradient\n",
    "                optimizer.zero_grad()\n",
    "                for s in range(len(examples)):\n",
    "                    model_inputs = self.tokenizer([examples[s]], return_tensors=\"pt\", padding=False).to(self.model.device)\n",
    "    \n",
    "                    # compute steered target\n",
    "                    if self.target_module == \"residual\":\n",
    "                        hidden_states = self.model(model_inputs[\"input_ids\"], output_hidden_states=True).hidden_states\n",
    "                    elif self.target_module == \"attn\":\n",
    "                        hidden_states = self.model(model_inputs[\"input_ids\"], output_attentions=True).attentions\n",
    "                    else:\n",
    "                        raise ValueError(\"target_module must be 'residual' or 'attn'\")\n",
    "                    target = hidden_states[self.target_layer_idx][:, self.target_token_idxs, :]\n",
    "                    loss = -(target-self.unsteered_targets[s]).norm(dim=1).pow(power).sum().pow(1/self.q)\n",
    "                    loss.backward()\n",
    "                \n",
    "                # project gradient to subspace orthogonal to previous learned vectors (if orthogonal_vectors is True)\n",
    "                if orthogonal_vectors:\n",
    "                    with torch.no_grad():\n",
    "                        bias.grad = project_orthogonal_subspace(\n",
    "                            bias.grad, \n",
    "                            self.learned_vectors, \n",
    "                            normalization\n",
    "                        )\n",
    "                \n",
    "                # project gradient to tangent space of sphere\n",
    "                with torch.no_grad():\n",
    "                    bias.grad -= torch.dot(\n",
    "                        bias.grad, bias\n",
    "                    ) * bias / (normalization**2)\n",
    "\n",
    "                # step\n",
    "                optimizer.step()\n",
    "\n",
    "                # project steering vector to subspace orthogonal to previous learned vectors (if orthogonal_vectors is True)\n",
    "                if orthogonal_vectors:\n",
    "                    with torch.no_grad():\n",
    "                        bias.data = project_orthogonal_subspace(bias, self.learned_vectors, normalization)\n",
    "\n",
    "                # normalize\n",
    "                with torch.no_grad():\n",
    "                    bias.data = nn.functional.normalize(bias.data, dim=0) * normalization\n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    l_ = loss.detach().item()\n",
    "                losses.append(l_)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                self.learned_vectors[i,:] = bias.data.detach()\n",
    "            losses_all.append(losses)\n",
    "            \n",
    "        self.losses_all = losses_all\n",
    "        pass\n",
    "\n",
    "    def set_steering_vector(self, i):\n",
    "        with torch.no_grad():\n",
    "            self.bias.data = self.learned_vectors[i,:]\n",
    "        pass\n",
    "    \n",
    "    def zero_steering_vector(self):\n",
    "        if self.bias is not None:\n",
    "            with torch.no_grad():\n",
    "                self.bias.data = torch.zeros(self.bias.data.shape, device=self.model.device)\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "magic_args,-all",
   "formats": "py,../nbdev//ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
